{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification of Drug Reviews\n",
    "\n",
    "Abydos can be helpful in general machine learning tasks like text classification. The following notebook demonstrates how Abydos's phonetic algoriths, string fingerprint functions, and q-grams can squeeze a little extra accuracy out of a text classification task.\n",
    "\n",
    "The text classification task below uses customer review text to predict the condition for which the drug in question was prescribed. No other data (the drug name, for example) is used in this task.\n",
    "\n",
    "### Caveats\n",
    "This is a toy problem. I have taken a dataset that was already divided into training & test sets and used the test set for validation, not as a genuine test set. On the other hand, I haven't done much hyperparameter tuning. Indeed, all of the classifiers used below have identical parameters: `LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337)`.\n",
    "\n",
    "However, Abydos was used in a [winning submission](https://www.kaggle.com/c/anlp-2015-classification-assignment/leaderboard) to a Kaggle (InClass) competition in UC Berkeley's 2015 Applied NLP course. The same [notebook](https://gist.github.com/chrislit/3852eed7cce4b3544db2) (but with its Pseudo-SSK classifier disabled due to memory requirements) was applied to [the following year's competition](https://www.kaggle.com/c/anlp-2016-classification-assignment/leaderboard), after the competition deadline, and beat that year's leader (0.89535 to 0.89369) without any tuning. So... Abydos can be useful in generalizing text classification tasks.\n",
    "\n",
    "### Imports\n",
    "\n",
    "We start by importing from standard libraries, Pandas, Abydos, scikit-learn (for the ML algorithms/pipeline), NLTK (for a tokenizer & stopword corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import os\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from abydos.phonetic import double_metaphone, soundex\n",
    "from abydos.fingerprint import skeleton_key, omission_key\n",
    "from abydos.qgram import QGrams\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english')) | set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Transformer from\n",
    "# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "# This pulls a single column from a supplied pandas dataframe for classification.\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.columns]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, if the dataset isn't already present, we download it to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('drugsComTrain_raw.tsv'):\n",
    "    from io import BytesIO\n",
    "    from zipfile import ZipFile\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    resp = urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\")\n",
    "    zipfile = ZipFile(BytesIO(resp.read()))\n",
    "    zipfile.extract('drugsComTrain_raw.tsv')\n",
    "    zipfile.extract('drugsComTest_raw.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a pair of cleanup functions are defined for the files above.\n",
    "\n",
    "From the DataFrame's review field, we remove surrounding quotes, unescape HTML escapes, lowercase, strip stopwords, and apply NLTK's `word_punct()` tokenizer.\n",
    "\n",
    "From its condition field, we combine a number of conditions into supercategories. (NB: No offense is intended if I've miscategorized any of these diagnoses or if their conflation is inappropriate. And in some cases, these conditions were conflated because they employ the same drugs.) All other conditions are tagged as `''` for later removal of these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    review = review.strip('\"')\n",
    "    review = html.unescape(review)\n",
    "\n",
    "    review = review.lower()\n",
    "    review = ' '.join([_ for _ in wordpunct_tokenize(review) if _ not in stopwords])\n",
    "    \n",
    "    return review\n",
    "\n",
    "def clean_condition(condition):\n",
    "    if not isinstance(condition, str):\n",
    "        return ''\n",
    "\n",
    "    if 'Pain' in condition:\n",
    "        condition = 'Pain'\n",
    "    elif condition in {'Insomnia', 'Narcolepsy'} or 'Sleep' in condition:\n",
    "        condition = 'Sleep'\n",
    "    elif condition in {'Weight Loss', 'Obesity'}:\n",
    "        condition = 'Weight'\n",
    "    elif condition in {'Depression', 'Anxiety', 'Bipolar Disorde', 'Anxiety and Stress'\n",
    "                       'Panic Disorde', 'Generalized Anxiety Disorde', 'Schizophrenia',\n",
    "                       'Major Depressive Disorde',}:\n",
    "        condition = 'Mental Health'\n",
    "    elif condition in {'Birth Control', 'Emergency Contraception', 'Menstrual Disorders'}:\n",
    "        condition = 'Contraception'\n",
    "    elif 'Headache' in condition or 'Migraine' in condition:\n",
    "        condition = 'Headache'\n",
    "    elif condition in {'Acne', 'Rosacea', 'Eczema'}:\n",
    "        condition = 'Dermatalogical'\n",
    "    else:\n",
    "        condition = ''\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the training & test sets are read into a DataFrame and pre-processed as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the TSVs into a DataFrame\n",
    "drug_train = pd.read_csv('drugsComTrain_raw.tsv', sep='\\t', index_col=0, usecols=[0,1,2,3])\n",
    "drug_test = pd.read_csv('drugsComTest_raw.tsv', sep='\\t', index_col=0, usecols=[0,1,2,3])\n",
    "\n",
    "# Clean the review field\n",
    "drug_train.review = drug_train.review.apply(clean_review)\n",
    "drug_test.review = drug_test.review.apply(clean_review)\n",
    "\n",
    "# Clean the condition field (condense some classes)\n",
    "drug_train.condition = drug_train.condition.apply(clean_condition)\n",
    "drug_test.condition = drug_test.condition.apply(clean_condition)\n",
    "\n",
    "# Drop records that aren't among the 7 condition classes we will consider\n",
    "drug_train = drug_train[drug_train.condition != '']\n",
    "drug_test = drug_test[drug_test.condition != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drug_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a dictionary to hold accuracy data and define a function to train a model on the training set, test it on the test set, and store & report the resulting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "def test_pipeline(pipeline, name):\n",
    "    model = pipeline.fit(drug_train, drug_train.condition)\n",
    "    drug_test['prediction'] = model.predict(drug_test)\n",
    "    acc = sum(drug_test['prediction']==\n",
    "              drug_test['condition'])/len(drug_test)\n",
    "    accuracies[name] = acc\n",
    "    print('Accuracy: {:0.3f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, here is a straightfoward classifier on unigrams from the review text. It achieves an already-enviable 95.139% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_plain = Pipeline([\n",
    "    ('extract', ColumnExtractor('review')),\n",
    "    ('vectorize', TfidfVectorizer(sublinear_tf=True, norm='l2', lowercase=False)),\n",
    "    ('classifier', LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337))\n",
    "])\n",
    "test_pipeline(pipeline_plain, 'plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run each word of the review text through Soundex and through Double Metaphone (separately, not in series) and store the results in new DataFrame columns. And then we run the results through their own pipelines and get somewhat-disappointing results of 92.588% and 93.947% accuracies, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train['soundex'] = drug_train.review.apply(lambda review:\n",
    "                                                ' '.join(soundex(word) for\n",
    "                                                         word in review.split()))\n",
    "\n",
    "drug_train['dmetaphone'] = drug_train.review.apply(lambda review:\n",
    "                                                   ' '.join(double_metaphone(word)[0] for\n",
    "                                                            word in review.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_test['soundex'] = drug_test.review.apply(lambda review:\n",
    "                                              ' '.join(soundex(word) for\n",
    "                                                       word in review.split()))\n",
    "\n",
    "drug_test['dmetaphone'] = drug_test.review.apply(lambda review:\n",
    "                                                 ' '.join(double_metaphone(word)[0] for\n",
    "                                                          word in review.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_soundex = Pipeline([\n",
    "    ('extract', ColumnExtractor('soundex')),\n",
    "    ('vectorize', TfidfVectorizer(sublinear_tf=True, norm='l2', lowercase=False)),\n",
    "    ('classifier', LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337))\n",
    "])\n",
    "test_pipeline(pipeline_soundex, 'soundex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dmetaphone = Pipeline([\n",
    "    ('extract', ColumnExtractor('dmetaphone')),\n",
    "    ('vectorize', TfidfVectorizer(sublinear_tf=True, norm='l2', lowercase=False)),\n",
    "    ('classifier', LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337))\n",
    "])\n",
    "test_pipeline(pipeline_dmetaphone, 'double metaphone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the Skeleton and Omission keys of each word of the review text and store the results in new DataFrame columns. And then we run the results through their own pipelines and get more encouraging results of 95.075% and 95.075% accuracy, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train['skeleton'] = drug_train.review.apply(lambda review:\n",
    "                                                 ' '.join(skeleton_key(word) for word in\n",
    "                                                          review.split()))\n",
    "drug_train['omission'] = drug_train.review.apply(lambda review:\n",
    "                                                 ' '.join(omission_key(word) for word in\n",
    "                                                          review.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_test['skeleton'] = drug_test.review.apply(lambda review:\n",
    "                                               ' '.join(skeleton_key(word) for word in\n",
    "                                                        review.split()))\n",
    "drug_test['omission'] = drug_test.review.apply(lambda review:\n",
    "                                               ' '.join(omission_key(word) for word in\n",
    "                                                        review.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_skeleton = Pipeline([\n",
    "    ('extract', ColumnExtractor('skeleton')),\n",
    "    ('vectorize', TfidfVectorizer(sublinear_tf=True, norm='l2', lowercase=False)),\n",
    "    ('classifier', LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337))\n",
    "])\n",
    "test_pipeline(pipeline_skeleton, 'skeleton key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_omission = Pipeline([\n",
    "    ('extract', ColumnExtractor('omission')),\n",
    "    ('vectorize', TfidfVectorizer(sublinear_tf=True, norm='l2', lowercase=False)),\n",
    "    ('classifier', LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337))\n",
    "])\n",
    "test_pipeline(pipeline_omission, 'omission key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another option, we retrieve character-wise 4-grams and 5-grams, storing them as dictionaries within their own column. This gives a nice improvement over the baseline, with 96.020% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train['qgrams'] = drug_train.review.apply(lambda review:\n",
    "                                               dict(QGrams(review, 4, start_stop='') +\n",
    "                                                    QGrams(review, 5, start_stop='')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_test['qgrams'] = drug_test.review.apply(lambda review:\n",
    "                                             dict(QGrams(review, 4, start_stop='') +\n",
    "                                                  QGrams(review, 5, start_stop='')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_qgrams = Pipeline([\n",
    "    ('extract', ColumnExtractor('qgrams')),\n",
    "    ('vectorize', DictVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(sublinear_tf=True, norm='l2')),\n",
    "    ('classifier', LinearSVC(loss='hinge', C=1, max_iter=2000, random_state=1337))\n",
    "])\n",
    "test_pipeline(pipeline_qgrams, 'q-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we throw all the pipelines together with a voting classifier, minus the worst performing (Soundex) pipeline. We also add weights to bias strongly towards the best performing (q-grams) pipeline. And the resulting pipeline should both generalize well and beat someone using the plain pipeline.\n",
    "\n",
    "Naturally, there is much room for improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_voting = VotingClassifier([\n",
    "    ('plain', pipeline_plain),\n",
    "    # ('soundex', pipeline_soundex),\n",
    "    ('dmetaphone', pipeline_dmetaphone),\n",
    "    ('skeleton', pipeline_skeleton),\n",
    "    ('omission', pipeline_omission),\n",
    "    ('qgrams', pipeline_qgrams),\n",
    "], weights=[1, 1, 1, 1, 2.5])\n",
    "test_pipeline(pipeline_voting, 'voting')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
